{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ygrZd3dmu5"
      },
      "source": [
        "# Classifying Exoplanets\n",
        "\n",
        "In this notebook, we'll continue improving our models for exoplanet classification!\n",
        "\n",
        "We'll be:\n",
        "*   Preprocessing the Dataset similar to before\n",
        "*   Implementing more modern and complex machine learning architectures to see which one performs best!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPDgkzsdguU"
      },
      "source": [
        "## Exoplanet Classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Previously, we were able to visualize and augment the dataset from Kepler. Now that we better understand the data that we're working with, we can begin to dive into more complex architectures to classify exoplanet stars, and the difficulties faced when doing so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmzc3C9XkIKL"
      },
      "source": [
        "**IMPORTANT**: Make sure you've got 'Change Runtime Type' set to **GPU**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjzYKqcweS2F",
        "cellView": "form"
      },
      "source": [
        "#@title Run this code to get started\n",
        "%tensorflow_version 2.x\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import  metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train.LABEL = df_train.LABEL -1\n",
        "df_test = pd.read_csv('exoTest.csv')\n",
        "df_test.LABEL = df_test.LABEL - 1\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def analyze_results(model, train_X, train_y, test_X, test_y):\n",
        "    \"\"\"\n",
        "    Helper function to help interpret and model performance.\n",
        "\n",
        "    Args:\n",
        "    model: estimator instance\n",
        "    train_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model training.\n",
        "    train_y : array-like of shape (n_samples,)\n",
        "    Target values for model training.\n",
        "    test_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model testing.\n",
        "    test_y : array-like of shape (n_samples,)\n",
        "    Target values for model testing.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Model Results\")\n",
        "    print(\"\")\n",
        "    print(\"Training:\")\n",
        "    if type(model) == keras.engine.sequential.Sequential:\n",
        "      train_predictions = model.predict(train_X)\n",
        "      train_predictions = (train_predictions > 0.5)\n",
        "      cm = confusion_matrix(train_y, train_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - TestData')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      plt.close()\n",
        "      ConfusionMatrixDisplay.from_estimator(model,train_X,train_y)\n",
        "      plt.show()\n",
        "\n",
        "    print(\"Testing:\")\n",
        "    if type(model) == keras.engine.sequential.Sequential:\n",
        "      test_predictions = model.predict(test_X)\n",
        "      test_predictions = (test_predictions > 0.5)\n",
        "      cm = confusion_matrix(test_y, test_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - TestData')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      ConfusionMatrixDisplay.from_estimator(model,test_X,test_y)\n",
        "\n",
        "def reset(train,test):\n",
        "    train_X = train.drop('LABEL', axis=1)\n",
        "    train_y = train['LABEL'].values\n",
        "    test_X = test.drop('LABEL', axis=1)\n",
        "    test_y = test['LABEL'].values\n",
        "    return train_X,train_y,test_X,test_y\n",
        "\n",
        "train_X,train_y,test_X,test_y = reset(df_train, df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAc67kxQftEV"
      },
      "source": [
        "Remember that `df_train` and `df_test` are the Pandas data frames that store our training and test datapoints. Similar to before, we'll now augment the data before exploring more modern, complex machine learning architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6yRtFD1pYP7d"
      },
      "source": [
        "#@title Run this code to preprocess data\n",
        "# Helper functions that we can run for the three augmentation functions that will be used, but not explroed in depth\n",
        "\n",
        "def smote(a,b):\n",
        "    model = SMOTE()\n",
        "    X,y = model.fit_resample(a, b)\n",
        "    return X,y\n",
        "\n",
        "def savgol(df1,df2):\n",
        "    x = savgol_filter(df1,21,4,deriv=0)\n",
        "    y = savgol_filter(df2,21,4,deriv=0)\n",
        "    return x,y\n",
        "\n",
        "def fourier(df1,df2):\n",
        "    train_X = np.abs(np.fft.fft(df1, axis=1))\n",
        "    test_X = np.abs(np.fft.fft(df2, axis=1))\n",
        "    return train_X,test_X\n",
        "\n",
        "def norm(df1,df2):\n",
        "    train_X = normalize(df1)\n",
        "    test_X = normalize(df2)\n",
        "    return train_X,test_X\n",
        "\n",
        "def robust(df1,df2):\n",
        "    scaler = RobustScaler()\n",
        "    train_X = scaler.fit_transform(df1)\n",
        "    test_X = scaler.transform(df2)\n",
        "    return train_X,test_X\n",
        "\n",
        "fourier_train_X, fourier_test_X = fourier(train_X, test_X)\n",
        "savgol_train_X, savgol_test_X = savgol(fourier_train_X, fourier_test_X)\n",
        "norm_train_X, norm_test_X = norm(savgol_train_X,savgol_test_X)\n",
        "robust_train_X, robust_test_X = robust(norm_train_X, norm_test_X)\n",
        "smote_train_X,smote_train_y = smote(robust_train_X, train_y)\n",
        "\n",
        "# Here we're adding the generated, augmented data onto the testing data\n",
        "aug_train_X, new_X_test_data, aug_train_y, new_y_test_data = train_test_split(smote_train_X, smote_train_y, test_size=0.3)\n",
        "aug_test_X = np.concatenate((robust_test_X, new_X_test_data), axis=0)\n",
        "aug_test_y = np.concatenate((test_y, new_y_test_data), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQPCRgpZVpW"
      },
      "source": [
        "Awesome! Now we'll have access to the augmented dataset as `aug_train_X`, `aug_text_X`, `aug_train_y`, and `aug_test_y`.\n",
        "\n",
        "(For further exploration and model comparison based off different datasets, you can explore the code block from above to access the different versions of the augmented data. For instance, what happens if we use the raw data? Normalized data?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw7FwS0_W7Bx"
      },
      "source": [
        "## Milestone 1: MLP\n",
        "\n",
        "Let's start with neural nets!\n",
        "\n",
        "MLP stands for Multi-layer Perceptron, a specific kind of simple neural network. Thankfully, this is something that Sklearn supports, and it's already imported as MLPClassifier.\n",
        "\n",
        "\n",
        "![visual](https://s3.amazonaws.com/stackabuse/media/intro-to-neural-networks-scikit-learn-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmbSCBCH2I9p"
      },
      "source": [
        "#### Step 1: Create our model\n",
        "\n",
        "Let's complete this by using a `MLPClassifier` model imported by the sklearn package. You can view the original documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Let's create a model with:\n",
        "1. One hidden layer with 10 units\n",
        "2. random_state = 1\n",
        "3. 300 max iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVs2i_C_F1o"
      },
      "source": [
        "# Create an MLP model (will train later)\n",
        "\n",
        "model = None  # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXj7fzfkXuQ9"
      },
      "source": [
        "Now, train your model using `aug_train_X` and `aug_train_y`, and analyze its accuracy and confusion matrix!\n",
        "\n",
        "You have access to all the methods used in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5okHo29ZXt13"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdW3vidfZQxg"
      },
      "source": [
        "**Discuss:** Were the results what you were expecting? Why or why not? How does it compare with past model results?\n",
        "\n",
        "What might be any potential issues that we might run into?\n",
        "\n",
        "**Hint:** What are some downsides to a traditional MLP? What would happen if we shifted the data left or right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYzv7-ZvNMb"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform when tested on the original dataset? (`train_X`, `test_X`, `train_y`, `test_y`) How does it perform when trained on the original dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQvSf0VOvRxe"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF34weGoClK"
      },
      "source": [
        "## Milestone 2: Neural Networks (Tensorflow and Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbCq5LpQgVkF"
      },
      "source": [
        "Now let's do what we did before, but using `tensorflow` and `keras`. These libraries will be crucial as they will allow us to create more complex models.\n",
        "\n",
        "We'll start by creating a similar model using these new packages.\n",
        "\n",
        "We'll be using a `Sequential` model in order to act as a \"list of layers\", which we will define to match our previous example. Later, we'll use it to build more complex, advanced models. More information can be found [here](https://keras.io/api/layers/).\n",
        "\n",
        "1. Add a `Dense` layer with 10 hidden units and a ReLU activation function. This layer also requires an `input_shape` parameter. What should the input shape be?\n",
        "\n",
        "2. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)\n",
        "\n",
        "**Discuss:** Why is there only 1 neuron in the final layer? Why do we have two layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shZOUCAbBJ0M"
      },
      "source": [
        "# Create a model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "\n",
        "#############\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "#############\n",
        "\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriqCvylFPSE"
      },
      "source": [
        "Now to check the details of your model, run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdk-_M3FTqA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QaGfpePBSKS"
      },
      "source": [
        "Now train and analyze your `model` like you did before! You'll need to specify these parameters to `fit`:\n",
        "\n",
        "1. `batch_size` = 64\n",
        "2. `epochs` = 20\n",
        "3. `verbose` = 1\n",
        "4. `validation_data` = (aug_test_X, aug_test_y)\n",
        "5. `shuffle` = True\n",
        "\n",
        "Save the history of the model as it trains or \"fits\" the data.\n",
        "\n",
        "Hint:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "history = model.fit(#YOUR CODE HERE)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Y1kOWtnOTv"
      },
      "source": [
        "# YOUR CODE HERE to train the model\n",
        "\n",
        "####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIBRY5RnbrMY"
      },
      "source": [
        "Now we will see how to view the performance of the model as it trained over time!\n",
        "\n",
        "In addition, we still want to be able to plot the confusion matrix of the model to check for performance and potential class biases. Enter code to analyze the model in the empty portion of the codeblock below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDly6NJObeiP"
      },
      "source": [
        "performance = model.evaluate(aug_test_X, aug_test_y, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "##############\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrQWbPYhfwZ"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`train_X`, `test_X`, `train_y`, `test_y`) How does it perform when trained on the original dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlXvkwTEhpNw"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_rRs1jTjDw"
      },
      "source": [
        "## Milestone 3: Covolutional Neural Network (CNN)\n",
        "\n",
        "One potential fault of our previous approach is memorizing the placement of specific patterns in the data. Although we were able to achieve great levels of accuracy, we might benefit from an architecture that can make decisions based on patterns no matter where they occur in the sample - for example, if we started measuring flux earlier or later!\n",
        "\n",
        "This is something that CNNs excel at. Most CNN architectures are set up to work with two dimensional inputs such as images, so our approach will be a bit different in working with and creating a one-dimensional CNN. However, similar concepts apply as we'll be passing a filter accoss the each data point with respect to time.\n",
        "\n",
        "[Here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) is a link to learn more about convolutional neural nets, and [here's](https://poloclub.github.io/cnn-explainer/) an interactive demo to explore. Try talking through the image of a traditional CNN below!\n",
        "\n",
        "![](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQlAZ043qOo"
      },
      "source": [
        "First, we'll have to \"reshape\" our augmented data into a shape that can be fed into a 1-dimensional CNN. We've reshaped the training data below - please **reshape the testing data, too.**\n",
        "\n",
        "Note: No new information is created, but just the way the information is structured. Because of this, we should have the same number of values present overall with no modifications to the values themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMWiJnl735CQ"
      },
      "source": [
        "cnn_aug_train_X = np.expand_dims(aug_train_X, axis=2)\n",
        "cnn_aug_train_y = aug_train_y\n",
        "\n",
        "############\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "############\n",
        "\n",
        "cnn_train_X = np.expand_dims(train_X, axis=2)\n",
        "cnn_train_y = train_y\n",
        "\n",
        "############\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHMC1fJWsrM"
      },
      "source": [
        "What are the new shapes of the data? The new data is stored in variables: `cnn_aug_train_X`, `cnn_aug_test_X`, `cnn_aug_train_y`, `cnn_aug_test_y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcti4YgIXdN4"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5bXRoaCTIj"
      },
      "source": [
        "Awesome!\n",
        "\n",
        "Now, we'll be using a `Sequential` model to build up our CNN. Here's a suggestion for an architecture to start with:\n",
        "\n",
        "1. Add a `Conv1D` layer with 8 output filters, kernal size of 5, relu activation function, and padding = 'same'. This layer also requires an `input_shape` parameter. Does the defined input_shape make sense?\n",
        "\n",
        "2. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "3. Add a `Conv1D` layer with 16 output filters, kernal size of 3, relu activation function, and padding = 'same'.\n",
        "\n",
        "4. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "5. Add a `Flatten` layer.\n",
        "\n",
        "6. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)\n",
        "\n",
        "Discuss: Why is there only 1 neuron in the final layer? Why do we have the same loss function and metrics as the network before if we're using two different architectures?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmW5k1otDu79"
      },
      "source": [
        "# Create model\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "input_shape = [3197, 1]\n",
        "\n",
        "#######TODO#########\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "####################\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDT84eyDGHY"
      },
      "source": [
        "Now train the model like we did before! (Make sure to use the newly formatted data instead. This will include `cnn_aug_train_X`, `cnn_aug_test_X`, `cnn_aug_train_y`, and `cnn_aug_test_y`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqX49x9pDvql"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "#######TODO#########\n",
        "\n",
        "#YOUR CODE HERE\n",
        "\n",
        "####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StKM_5W6uOYZ"
      },
      "source": [
        "Once again, let's analyze the model's performance over time and the final confusion matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C4uq3IUEHbo"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92tn05OBh0S2"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`cnn_train_X`, `cnn_test_X`, `cnn_train_y`, `cnn_test_y`, these are the same original values but resized to work with the new architecture) How does it perform when trained on the original dataset?\n",
        "\n",
        "**Hint:** Try out new layers like a \"Dropout\" layer. More complex architectures often lead to better results as well. Try increasing the number of channels in each convolution layer or adding more layers in general!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVu5mBrwh1mA"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWrP7kYgdVo"
      },
      "source": [
        "## Milestone 4: Optional Exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svt0LBR34_WT"
      },
      "source": [
        "Congratulations! You've learned to visually analyze and refine raw satellite data, and built a top-of the line model that accurately detects exoplanet stars vs. non-exoplanet stars. This is critical to exoplanet hunting because it allows planetary hunters to focus on studying the exoplanets we've discovered, and analyzing them for mass, habitability, etc.\n",
        "\n",
        "Remember that in our original dataset, exoplanet stars accounted for less than 1 % of all samples collected. In notebooks 2 and 3, we used machine learning to automatically identify likely exoplanet stars, dramatically reducing the time and effort needed to find them!\n",
        "\n",
        "This pipeline can be used to help aid the search of exoplanets for the incoming, raw data. It might even lead to new planetary discoveries as space exploration continues! Try exploring more raw, unprocessed NASA data [here](https://www.nasa.gov/kepler/education/getlightcurves).\n",
        "\n",
        "Of course, the more data, the better. This model and pipeline can be further improved with future iterations of new data and architectures. If you decide to go planet hunting, have fun on your new adventure!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlouEr3ggRN"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "Modify past model architectures or create new ones! See how they perform with the augmented dataset vs the original dataset! We can even explore different pre-processing techniques to help imporve the quality of our data! How will the new models, modifications, or pre-processing techniques compare to our past results? Which is the best solution?\n",
        "\n",
        "**Hint**\n",
        "\n",
        "If looking for inspiration, go to Tensorflow and check out different kinds of layers and activation functions! You can find the original documentation [here](https://keras.io/api/layers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfGsXkpw00u"
      },
      "source": [
        "#YOUR CODE HERE - happy planet hunting!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}